---
title: "Diagnostics and<br/>Model Evaluation"
subtitle: "Lecture 03"
author: "Dr. Colin Rundel"
footer: "Sta 344 - Fall 2022"
format:
  revealjs:
    theme: slides.scss
    transition: fade
    slide-number: true
    self-contained: true
execute:
  echo: true
---

```{r setup, include=FALSE}
library(tidyverse)

#source("../util-crps.R")

knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width=7,
  fig.height=3.8,
  out.width="\\textwidth",
  fig.align="center",
  echo=TRUE,
  warning=FALSE
)

ggplot2::theme_set(ggplot2::theme_bw())
```


# Some more linear models

## Linear model and data

```{r echo=FALSE}
set.seed(01232018)
n = 100

d = data_frame(
  x = 1:n,
  y = arima.sim(n=100, list(ar=0.9,sq=1)) %>% as.numeric() + x * 0.07
)
```

```{r}
#| fig.height: 4
ggplot(d, aes(x=x,y=y)) + 
  geom_point() + 
  geom_smooth(method="lm", color="blue", se = FALSE)
```

## Linear model

```{r}
l = lm(y ~ x, data=d)

summary(l)
```


## Bayesian model (brms)

```{r}
#| message: false
#| warning: false
( b = brms::brm(
    y ~ x, data=d,
    prior = c(
      brms::prior(normal(0, 100), class = Intercept),
      brms::prior(normal(0, 10),  class = b),
      brms::prior(cauchy(0, 2),   class = sigma)
    ),
    silent = 2, refresh = 0
  )
)
```

## Parameter estimates

```{r}
plot(b)
```

## tidybayes

```{r echo=TRUE}
b_post = b |>
  tidybayes::gather_draws(b_Intercept, b_x, sigma) |>
  group_by(.variable, .chain)

head(b_post)
tail(b_post)
```


## Posterior plots

```{r echo=TRUE, fig.height=4}
b_post |>
  ggplot(aes(fill=as.factor(.chain), group=.chain, x=.value)) +
  geom_density(alpha=0.33, color=NA) +
  facet_wrap(~.variable, scales = "free")
```

## Trace plots

```{r echo=TRUE, fig.height=3.8} 
b_post %>% 
  ggplot(aes(x=.iteration, y=.value, color=as.factor(.chain))) +
  geom_line(alpha=0.5) +
  facet_grid(.variable~.chain, scale="free_y") +
  geom_smooth(method="loess") + labs(color="chain")
```


## Credible Intervals

```{r echo=TRUE}
(b_ci = tidybayes::mean_hdi(b_post, .value, .width=c(0.8, 0.95)))
```

## Aside - `mean_qi` vs `mean_hdi`

These differ in the use of the quantile interval vs. the highest-density interval.

. . .

```{r echo=FALSE}
set.seed(1234)
ci_ex = data_frame(
  dist_1 = rnorm(10000),
  dist_2 = c(rnorm(5000, 2), rnorm(5000, -2))
) %>% 
  mutate_all(function(x) (x-min(x))/max(x) ) %>%
  tidyr::gather(dist, x) 

tmp = rbind(
  ci_ex %>% group_by(dist) %>% tidybayes::mean_qi( x, .width=c(0.5,0.95)) %>% mutate(method="qi"),
  ci_ex %>% group_by(dist) %>% tidybayes::mean_hdi(x, .width=c(0.5,0.95)) %>% mutate(method="hpd")
) %>%
  group_by(method,dist, .width) %>%
  mutate(region = 1:n()) %>%
  tidyr::gather(type, bound, .lower, .upper)

  ggplot(tmp, aes(x=x)) +
    geom_density(data=ci_ex, aes(group=dist), fill="grey", color="black") + 
    geom_vline(aes(color=as.factor(.width), xintercept=bound), size=1.5) +
    facet_grid(method~dist, scale="free_x") +
    ylim(0,4.75) +
    labs(color="prob")
```

## Caterpillar Plots

```{r echo=TRUE, fig.height=4}
b_ci %>%
  ggplot(aes(x=.value, y=.chain, color=as.factor(.chain))) + 
  facet_grid(.variable~.) +
  tidybayes::geom_pointintervalh() +
  ylim(0.5,4.5)
```


# Predictions

## lm predictions

:::: {.columns}
::: {.column width='50%'}
```{r}
(l_pred = predict(l, se.fit = TRUE))
```
:::

::: {.column width='50%'}
```{r}
d %>%
  mutate(pred = l_pred$fit, pred_se = l_pred$se.fit) |>
  ggplot(aes(x=x,y=y)) + 
  geom_point() +
  geom_line(aes(y=pred), col="blue", size=1.5) +
  geom_ribbon(aes(ymin=pred-1.96*pred_se, ymax=pred+1.96*pred_se), col="blue", fill="blue", alpha=0.33)
```
:::
::::




## brms predictions

:::: {.columns}
::: {.column width='50%'}
```{r}
(b_pred = predict(b))
```
:::

::: {.column width='50%'}
```{r}
d %>%
  bind_cols(b_pred) |>
  ggplot(aes(x=x,y=y)) + 
  geom_point() +
  geom_line(aes(y=Estimate), col="red", size=1.5) +
  geom_ribbon(aes(ymin=Q2.5, ymax=Q97.5), col='red', fill='red', alpha=0.33)
```

# Why are the intervals different?

## Raw predictions

```{r}
dim( brms::posterior_predict(b) )
dim( brms::posterior_epred(b) )
```


## Tidy raw predictions

::::: {.small}
:::: {.columns}
::: {.column width='50%'}
```{r}
( b_post_pred = tidybayes::predicted_draws(b, newdata=d) )
```
:::

::: {.column width='50%'}
```{r}
( b_post_epred = tidybayes::epred_draws(b, newdata=d) )
```
:::
::::
:::::



## Posterior predictions vs Expected posterior predictions

::::: {.small}
:::: {.columns}
::: {.column width='50%'}
```{r echo=TRUE}
b_post_pred |>
  filter(.draw <= 25) |>
  ggplot(aes(x=x,y=y)) +
    geom_point() +
    geom_line(aes(y=.prediction, group=.draw), alpha=0.33)
```
:::

::: {.column width='50%'}
```{r echo=TRUE}
b_post_epred |>
  filter(.draw <= 25) |>
  ggplot(aes(x=x,y=y)) +
    geom_point() +
    geom_line(aes(y=.epred, group=.draw), alpha=0.33)
```
:::
::::
:::::


## Credible intervals

::::: {.small}
:::: {.columns}
::: {.column width='50%'}
```{r echo=TRUE}
b_post_pred |>
  ggplot(aes(x=x, y=y)) +
    geom_point() +
    tidybayes::stat_lineribbon(aes(y=.prediction), alpha=0.25)
```
:::

::: {.column width='50%'}
```{r echo=TRUE}
b_post_epred |>
  ggplot(aes(x=x, y=y)) +
    geom_point() +
    tidybayes::stat_lineribbon(aes(y=.epred), alpha=0.25)
```
:::
::::
:::::

## Posterior predictive checks

```{r}
brms::pp_check(b, ndraws = 25)
```

  
## Residuals - lm

```{r}
l |>
  broom::augment() |>
  ggplot(aes(x=x, y=.resid)) +
    geom_point() +
    geom_hline(yintercept=0, color='grey', linetype=2)
```

  
## Residuals - brms

```{r}
b %>%
  tidybayes::residual_draws(newdata=d) |>
  ggplot(aes(x=x, y=.residual, group=x)) +
    tidybayes::stat_pointinterval() +
    geom_hline(yintercept = 0, color='grey', linetype=2)
```


# Model Evaluation

## Model assessment?

If we think back to our first regression class, one common option is $R^2$ which gives us the variability in $Y$ explained by our model.

Quick review:
  
. . . 

$$ 
\underset{\text{Total}}{\sum_{i=1}^n \left(Y_i - \bar{Y}\right)^2} = \underset{\text{Model}}{\sum_{i=1}^n \left(\hat{Y}_i - \bar{Y}\right)^2} + \underset{\text{Error }}{\sum_{i=1}^n \left(Y_i - \hat{Y}_i\right)^2}
$$
  
. . .

$$
R^2 
  = \frac{SS_{model}}{SS_{total}}
  = \frac{\sum_{i=1}^n \left(\hat{Y}_i - \bar{Y}\right)^2}{\sum_{i=1}^n \left(Y_i - \bar{Y}\right)^2}
  = \frac{\text{Var}(\hat{\boldsymbol{Y}}) }{ \text{Var}({\boldsymbol{Y}}) }
  = \text{Cor}(\boldsymbol{Y}, \hat{\, color='grey', linetype=2), color='grey', linetype=2){Y}})^2 
$$
  
  
