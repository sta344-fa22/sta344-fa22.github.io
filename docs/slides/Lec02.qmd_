---
title: "Linear Models"
subtitle: "Lecture 01"
author: "Dr. Colin Rundel"
footer: "Sta 344 - Fall 2022"
format:
  revealjs:
    theme: slides.scss
    transition: fade
    slide-number: true
    self-contained: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  fig.width=7,
  fig.height=5,
  out.width="\\textwidth",
  fig.align="center",
  echo=FALSE,
  warning=FALSE,
  cache=TRUE
)

library(tidyverse)

ggplot2::theme_set(ggplot2::theme_bw())
```


# Linear Models

## Linear Models Basics

Pretty much everything we a going to see in this course will fall under the umbrella of either linear or generalized linear models. 

In previous classes most of your time has likely been spent with the simple iid case,

$$Y_i = \beta_0 + \beta_1 \, x_{i1} + \cdots + \beta_p \, x_{ip} + \epsilon_i $$
$$ \epsilon_i \sim N(0, \sigma^2) $$

these models can also be expressed simply as,

$$ Y_i \overset{iid}{\sim} N(\beta_0 + \beta_1 \, x_{i1} + \cdots + \beta_p \, x_{ip},~ \sigma^2) $$

## Linear model - matrix notation

We can also express using matrix notation as follows,

$$
\begin{aligned}
\underset{n \times 1}{\boldsymbol{Y}} = \underset{n \times p}{\boldsymbol{X}} \, \underset{p \times 1}{\boldsymbol{\beta}} + \underset{n \times 1}{\boldsymbol{\epsilon}} \\
\underset{n \times 1}{\boldsymbol{\epsilon}} \sim N(\underset{n \times 1}{\boldsymbol{0}}, \; \sigma^2 \underset{n \times n}{\mathbb{1}_n})
\end{aligned}
$$

or alternative as,

$$ 
\underset{n \times 1}{\boldsymbol{Y}} \sim N\left(\underset{n \times p}{\boldsymbol{X}} \, \underset{p \times 1}{\boldsymbol{\beta}},~  \sigma^2 \underset{n \times n}{\mathbb{1}_n}\right)
$$


## Multivariate Normal Distribution - Review

For an $n$-dimension multivate normal distribution with covariance $\boldsymbol{\Sigma}$ (positive semidefinite) can be written as

$$
\underset{n \times 1}{\boldsymbol{Y}} \sim N(\underset{n \times 1}{\boldsymbol{\mu}}, \; \underset{n \times n}{\boldsymbol{\Sigma}}) \text{ where } \{\boldsymbol{\Sigma}\}_{ij} = \rho_{ij} \sigma_i \sigma_j
$$

$$
\begin{pmatrix}
Y_1\\ Y_2\\ \vdots\\ Y_n
\end{pmatrix}
\sim N\left(
\begin{pmatrix}
\mu_1\\ \mu_2\\ \vdots\\ \mu_n
\end{pmatrix}, \,
\begin{pmatrix}
\rho_{11}\sigma_1\sigma_1 & \rho_{12}\sigma_1\sigma_2 & \cdots & \rho_{1n}\sigma_1\sigma_n \\
\rho_{21}\sigma_2\sigma_1 & \rho_{22}\sigma_2\sigma_2 & \cdots & \rho_{2n}\sigma_2\sigma_n\\
\vdots & \vdots & \ddots & \vdots \\
\rho_{n1}\sigma_n\sigma_1 & \rho_{n2}\sigma_n\sigma_2 & \cdots & \rho_{nn}\sigma_n\sigma_n \\
\end{pmatrix}
\right)
$$


## Multivariate Normal Distribution - Density

For the $n$ dimensional multivate normal given on the last slide, its density is given by

$$
(2\pi)^{-n/2} \; \det(\boldsymbol{\Sigma})^{-1/2} \; \exp{\left(-\frac{1}{2} \underset{1 \times n}{(\boldsymbol{Y}-\boldsymbol{\mu})'} \underset{n \times n}{\boldsymbol{\Sigma}^{-1}} \underset{n \times 1}{(\boldsymbol{Y}-\boldsymbol{\mu})}\right)} 
$$

and its log density is given by

$$
-\frac{n}{2} \log 2\pi - \frac{1}{2} \log \det(\boldsymbol{\Sigma}) - \frac{1}{2} \underset{1 \times n}{(\boldsymbol{Y}-\boldsymbol{\mu})'} \underset{n \times n}{\boldsymbol{\Sigma}^{-1}} \underset{n \times 1}{(\boldsymbol{Y}-\boldsymbol{\mu})}
$$


## Maximum Likelihood - $\boldsymbol{\beta}$ (iid)


## Maximum Likelihood - $\sigma^2$ (iid)



# A Quick Example

## Parameters -> Synthetic Data

Lets generate some simulated data where the underlying model is known and see how various regression procedures function.

$$ y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} +\epsilon_i $$ 
$$ \beta_0 = 0.7, \quad \beta_1 = 1.5, \quad \beta_2 = -2.2, \quad \beta_3 = 0.1 $$
$$ \epsilon_i \sim N(0,1) $$

. . .

```{r echo=TRUE}
set.seed(1234)
n = 100
beta = c(0.7,1.5,-2.2,0.1)
eps = rnorm(n)

d = data_frame(
  X1 = rt(n,df=5),
  X2 = rt(n,df=5),
  X3 = rt(n,df=5)
) %>%
  mutate(Y = beta[1] + beta[2]*X1 + beta[3]*X2 + beta[4]*X3 + eps)
```

## Model Matrix

```{r echo=TRUE}
X = model.matrix(~X1+X2+X3, d) 
tbl_df(X)
```

## Pairs plot

```{r echo=TRUE, out.width="0.8\\textwidth"}
GGally::ggpairs(d, progress = FALSE)
```


## Least squares fit

Let $\hat{\boldsymbol{Y}}$ be our estimate for $\boldsymbol{Y}$ based on our estimate of $\boldsymbol{\beta}$,
$$ \hat{\boldsymbol{Y}} = \hat{\beta}_0 + \hat{\beta}_1 \, \boldsymbol{X}_{1} + \hat{\beta}_2 \, \boldsymbol{X}_{2} + \hat{\beta}_3 \, \boldsymbol{X}_{3} = \boldsymbol{X}\, \hat{\boldsymbol{\beta}} $$

. . .

The least squares estimate, $\hat{\boldsymbol{\beta}}_{ls}$, is given by
$$ \underset{\boldsymbol{\beta}}{\text{argmin}} \sum_{i=1}^n \left( Y_i - \boldsymbol{X}_{i\cdot} \boldsymbol{\beta} \right)^2 $$

. . .

Previously we derived,
$$ \hat{\boldsymbol{\beta}}_{ls} = (\boldsymbol{X}' \boldsymbol{X})^{-1} \boldsymbol{X}' \, \boldsymbol{Y} $$

## Frequentist Fit

```{r echo=TRUE}
(beta_hat = solve(t(X) %*% X, t(X)) %*% d$Y)
```

. . .

```{r echo=TRUE}
l = lm(Y ~ X1 + X2 + X3, data=d)
l$coefficients
```

# Bayesian regression model

## Basics of Bayes


## brms

We will be using a package called [brms](https://paul-buerkner.github.io/brms/) for most of our Bayesian model fitting

- it has a convenient model specification syntax

- mostly sensible prior defaults

- supports most of the model types we will be exploring

- uses Stan behind the scenes


## brms + linear regression

```{r echo=TRUE}
b = brms::brm(Y ~ X1 + X2 + X3, data=d, chains = 2)
```

## Model results

```{r echo=TRUE}
b
```

##

```{r}
plot(b)
```


## Bayesian model specification (JAGS)

```{r echo=TRUE}
model = 
"model{
  # Likelihood
  for(i in 1:length(Y)){
    Y[i] ~ dnorm(mu[i], tau)
    mu[i] = beta[1] + beta[2]*X1[i] + beta[3]*X2[i] + beta[4]*X3[i]
  }

  # Prior for beta
  for(j in 1:4){
    beta[j] ~ dnorm(0,1/100)
  }

  # Prior for sigma / tau2
  tau ~ dgamma(1, 1)
  sigma2 = 1/tau
}"
```

## Compiling

\scriptsize
```{r echo=TRUE, message=FALSE}
m = rjags::jags.model(
  textConnection(model), 
  data = d, n.chains = 2
)
```

## Sampling

```{r echp=TRUE}
# Burnin 
update(m, n.iter=1000, progress.bar="none")

# Draw samples
samp = rjags::coda.samples(
  m, variable.names=c("beta","sigma2"), 
  n.iter=5000, progress.bar="none"
)
```


## Results

```{r echo=TRUE}
str(samp)
```

## CODA & mcmc objects

```{r echo=TRUE}
head(samp[[1]])
```

## CODA & mcmc objects - plotting

```{r echo=TRUE}
plot(samp)
```

## Tidy Bayes

```{r}
df = samp %>%
  tidybayes::gather_draws(beta[i], sigma2) %>%
  mutate(param = ifelse(is.na(i), .variable, paste0(.variable,"[",i,"]")))

df
```

## Tidy Bayes + ggplot - Traceplot

\scriptsize
```{r fig.height=4}
ggplot(df, aes(x=.iteration, y=.value, color=as.character(.chain))) +
  geom_line(alpha=0.5) +
  facet_wrap(~param, scale="free_y") +
  labs(color="Chain")
```

## Tidy Bayes + ggplot - Density plot

\scriptsize
```{r fig.height=4}
ggplot(df, aes(x=.value, fill=as.character(.chain))) +
  geom_density(alpha=0.5) +
  facet_wrap(~param, scale="free_x") +
  labs(fill="Chain")
```

## Comparing Approaches

\scriptsize

```{r}
pt_est = df %>% 
  filter(.chain == 1) %>%
  group_by(param) %>% 
  summarize(post_mean = mean(.value)) %>%
  ungroup() %>%
  mutate(
    truth = c(0.7, 1.5, -2.2, 0.1, 1),
    ols   = c(l$coefficients, var(l$residuals))
  ) %>%
  select(param, truth, ols, post_mean)

pt_est
```

## Comparing Approaches - code

```{r dens-plot, eval=FALSE}
ggplot(df, aes(x=.value, fill=as.character(.chain))) +
  geom_density(alpha=0.5) +
  facet_wrap(~param, scale="free_x") +
  geom_vline(
    data = tidyr::gather(pt_est, pt_est, value, -param), 
    aes(xintercept = value, color=pt_est)
  ) +
  labs(fill="Chain")
```
## Comparing Approaches - plot

```{r ref.label="dens-plot", echo=FALSE}
```







## Bayesian Model

Likelihood:

$$
\boldsymbol{Y} \,|\, \boldsymbol{\beta}, \, \sigma^2 \sim N(\boldsymbol{X}\boldsymbol{\beta},\, \sigma^2 \, {\mathbb{1}_n})
$$

. . .

Priors:
$$
\beta_i \sim N(0, \sigma^2_\beta)
\text{  or  } 
\boldsymbol{\beta} \sim N(\boldsymbol{0}, \sigma^2_\beta \, {\mathbb{1}_p})
$$

$$
\sigma^2 \sim \text{Inv-Gamma}(a,\,b)
$$

## Deriving the posterior

\footnotesize

$$ 
\begin{aligned}
\left[ \boldsymbol{\beta}, \sigma^2 \,|\, \boldsymbol{Y}, \boldsymbol{X} \right] 
  &= \frac{[\boldsymbol{Y} \,|\, \boldsymbol{X}, \boldsymbol{\beta}, \sigma^2]}{[\boldsymbol{Y} \,|\, \boldsymbol{X}]} [\boldsymbol{\beta}, \sigma^2] \\
  &\propto [\boldsymbol{Y} \,|\, \boldsymbol{X}, \boldsymbol{\beta}, \sigma^2] [\boldsymbol{\beta},\,\sigma^2] \\
  &\propto [\boldsymbol{Y} \,|\, \boldsymbol{X}, \boldsymbol{\beta}, \sigma^2] [\boldsymbol{\beta}\,|\,\sigma^2] [\sigma^2]
\end{aligned}
$$

. . . 

where, 

$$ 
f(\boldsymbol{Y} \,|\, \boldsymbol{X}, \boldsymbol{\beta}, \sigma^2) = 
\left(2\pi \sigma^2\right)^{-n/2} \exp\left( -\frac{1}{2\sigma^2} (\boldsymbol{Y}-\boldsymbol{X}\boldsymbol{\beta})'(\boldsymbol{Y}-\boldsymbol{X}\boldsymbol{\beta}) \right) 
$$

. . .

$$ 
f(\boldsymbol{\beta}\,|\, \sigma^2_\beta) = (2\pi \sigma^2_\beta)^{-p/2} \exp\left( -\frac{1}{2\sigma^2_\beta} \boldsymbol{\beta}'\boldsymbol{\beta} \right) 
$$

. . .

$$
f(\sigma^2 \,|\, a,\, b) = \frac{b^a}{\Gamma(a)} (\sigma^2)^{-a-1} \exp\left( -\frac{b}{\sigma^2} \right) 
$$


## Deriving the Gibbs sampler ($\sigma^2$ step)

\scriptsize
$$
\begin{aligned}
\left[ \boldsymbol{\beta}, \sigma^2 \,|\, \boldsymbol{Y}, \boldsymbol{X}  \right]  \propto
  &\left(2\pi \sigma^2\right)^{-n/2} \exp\left( -\frac{1}{2\sigma^2} (\boldsymbol{Y}-\boldsymbol{X}\boldsymbol{\beta})'(\boldsymbol{Y}-\boldsymbol{X}\boldsymbol{\beta}) \right) \\
  &(2\pi \sigma^2_\beta)^{-p/2} \exp\left( -\frac{1}{2\sigma^2_\beta} \boldsymbol{\beta}'\boldsymbol{\beta} \right) \\
  &\frac{b^a}{\Gamma(a)} (\sigma^2)^{-a-1} \exp\left( -\frac{b}{\sigma^2} \right) 
\end{aligned}
$$

## {.plain}


## Deriving the Gibbs sampler ($\boldsymbol{\beta}$ step)

\scriptsize
$$
\begin{aligned}
\left[ \boldsymbol{\beta}, \sigma^2 \,|\, \boldsymbol{Y}, \boldsymbol{X}  \right]  \propto
  &\left(2\pi \sigma^2\right)^{-n/2} \exp\left( -\frac{1}{2\sigma^2} (\boldsymbol{Y}-\boldsymbol{X}\boldsymbol{\beta})'(\boldsymbol{Y}-\boldsymbol{X}\boldsymbol{\beta}) \right) \\
  &(2\pi \sigma^2_\beta)^{-p/2} \exp\left( -\frac{1}{2\sigma^2_\beta} \boldsymbol{\beta}'\boldsymbol{\beta} \right) \\
  &\frac{b^a}{\Gamma(a)} (\sigma^2)^{-a-1} \exp\left( -\frac{b}{\sigma^2} \right) 
\end{aligned}
$$

## {.plain}

